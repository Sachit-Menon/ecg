{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import gc\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sys\n",
    "sys.path.insert(0, './preparation')\n",
    "import os\n",
    "\n",
    "# Keras imports\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, Dense, Flatten, Dropout,MaxPooling1D, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback,warnings, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/ubuntu/projects/ecg/'\n",
    "data_path = path + 'data/train/training/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data training set\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data training set\")        \n",
    "matfile = scipy.io.loadmat(data_path+'TrainData1000.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__globals__': [],\n",
       " '__header__': 'MATLAB 5.0 MAT-file, Platform: PCWIN64, Created on: Fri Dec 08 13:48:58 2017',\n",
       " '__version__': '1.0',\n",
       " 'trainData': array([[-59.99505362, -59.98524582, -59.97697805, ..., -59.22539085,\n",
       "         -59.11628773, -58.42976453],\n",
       "        [-59.99608979, -59.98861609, -59.98153551, ..., -56.95266751,\n",
       "         -56.11547729, -55.55837516],\n",
       "        [-59.99511224, -59.98552185, -59.972777  , ..., -45.71653897,\n",
       "         -44.46837574, -43.18280277],\n",
       "        ..., \n",
       "        [-59.9410233 , -59.81968743, -59.69713992, ..., -60.74308101,\n",
       "         -60.59376259, -60.60749349],\n",
       "        [-59.93932438, -59.81435612, -59.68827016, ..., -60.77285387,\n",
       "         -60.77969118, -60.58663034],\n",
       "        [-59.93985226, -59.81825768, -59.69636081, ..., -63.44466112,\n",
       "         -63.37986803, -63.34375053]]),\n",
       " 'trainLabels': array([[1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        ..., \n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [1, 0]], dtype=uint8)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loaddata(WINDOW_SIZE):    \n",
    "    '''\n",
    "        Load training/test data into workspace\n",
    "        \n",
    "        This function assumes you have downloaded and padded/truncated the \n",
    "        training set into a local file named \"trainingset.mat\". This file should \n",
    "        contain the following structures:\n",
    "            - trainset: NxM matrix of N ECG segments with length M\n",
    "            - traintarget: Nx4 matrix of coded labels where each column contains\n",
    "            one in case it matches ['A', 'N', 'O', '~'].\n",
    "        \n",
    "    '''\n",
    "    print(\"Loading data training set\")        \n",
    "    matfile = scipy.io.loadmat(data_path+'TrainData13000.mat')\n",
    "    X = matfile['trainData']\n",
    "    y = matfile['trainLabels']\n",
    "    \n",
    "    # Merging datasets    \n",
    "    # Case other sets are available, load them then concatenate\n",
    "    #y = np.concatenate((traintarget,augtarget),axis=0)     \n",
    "    #X = np.concatenate((trainset,augset),axis=0)     \n",
    "\n",
    "    X =  X[:,0:WINDOW_SIZE] \n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ResNet_model(WINDOW_SIZE):\n",
    "    #parameters based on from Rajpurkar et al\n",
    "    INPUT_FEAT = 1\n",
    "    OUTPUT_CLASS = 2 #normal/abnormal: \n",
    "\n",
    "    k = 1\n",
    "    p = True #pooling alternates every block\n",
    "    convfilt = 64\n",
    "    convstr = 1\n",
    "    kern_size = 16\n",
    "    poolsize = 2\n",
    "    poolstr  = 2\n",
    "    drop = 0.9\n",
    "    \n",
    "    input1 = Input(shape=(WINDOW_SIZE,INPUT_FEAT))\n",
    "    \n",
    "    #initial convblock\n",
    "    x = Conv1D(filters=convfilt,\n",
    "               kernel_size=kern_size,\n",
    "               padding='same',\n",
    "               strides=convstr,\n",
    "               kernel_initializer='he_normal')(input1)                \n",
    "    x = BatchNormalization()(x)        \n",
    "    x = Activation('relu')(x)  \n",
    "    \n",
    "    #convblock 2 and branches:\n",
    "    #main sequence\n",
    "    x1 =  Conv1D(filters=convfilt,\n",
    "               kernel_size=kern_size,\n",
    "               padding='same',\n",
    "               strides=convstr,\n",
    "               kernel_initializer='he_normal')(x)      \n",
    "    x1 = BatchNormalization()(x1)    \n",
    "    x1 = Activation('relu')(x1)\n",
    "    x1 = Dropout(drop)(x1)\n",
    "    x1 =  Conv1D(filters=convfilt,\n",
    "               kernel_size=kern_size,\n",
    "               padding='same',\n",
    "               strides=convstr,\n",
    "               kernel_initializer='he_normal')(x1)\n",
    "    x1 = MaxPooling1D(pool_size=poolsize,\n",
    "                      strides=poolstr)(x1)\n",
    "    #shortcut\n",
    "    x2 = MaxPooling1D(pool_size=poolsize,\n",
    "                      strides=poolstr)(x)\n",
    "    #combine both\n",
    "    x = keras.layers.add([x1, x2])\n",
    "    del x1,x2\n",
    "    \n",
    "    #repeated convblock creation\n",
    "    p = not p \n",
    "    for l in range(5):\n",
    "        \n",
    "        if (l%4 == 0) and (l>0):\n",
    "            k += 1\n",
    "            xshort = Conv1D(filters=convfilt*k,kernel_size=1)(x)\n",
    "        else:\n",
    "            xshort = x        \n",
    "        #main section       \n",
    "        x1 = BatchNormalization()(x)\n",
    "        x1 = Activation('relu')(x1)\n",
    "        x1 = Dropout(drop)(x1)\n",
    "        x1 =  Conv1D(filters=convfilt*k,\n",
    "               kernel_size=kern_size,\n",
    "               padding='same',\n",
    "               strides=convstr,\n",
    "               kernel_initializer='he_normal')(x1)        \n",
    "        x1 = BatchNormalization()(x1)\n",
    "        x1 = Activation('relu')(x1)\n",
    "        x1 = Dropout(drop)(x1)\n",
    "        x1 =  Conv1D(filters=convfilt*k,\n",
    "               kernel_size=kern_size,\n",
    "               padding='same',\n",
    "               strides=convstr,\n",
    "               kernel_initializer='he_normal')(x1)        \n",
    "        if p:\n",
    "            x1 = MaxPooling1D(pool_size=poolsize,strides=poolstr)(x1)                \n",
    "\n",
    "        #shortcut for ResNet\n",
    "        if p:\n",
    "            x2 = MaxPooling1D(pool_size=poolsize,strides=poolstr)(xshort)\n",
    "        else:\n",
    "            x2 = xshort       \n",
    "        # combine sides\n",
    "        x = keras.layers.add([x1, x2])\n",
    "        #to pool or not to pool\n",
    "        p = not p \n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x) \n",
    "    x = Flatten()(x)\n",
    "    out = Dense(OUTPUT_CLASS, activation='softmax')(x)\n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "class AdvancedLearnignRateScheduler(Callback):    \n",
    "    '''\n",
    "   # Arguments\n",
    "       monitor: quantity to be monitored.\n",
    "       patience: number of epochs with no improvement\n",
    "           after which training will be stopped.\n",
    "       verbose: verbosity mode.\n",
    "       mode: one of {auto, min, max}. In 'min' mode,\n",
    "           training will stop when the quantity\n",
    "           monitored has stopped decreasing; in 'max'\n",
    "           mode it will stop when the quantity\n",
    "           monitored has stopped increasing.\n",
    "   '''\n",
    "    def __init__(self, monitor='val_loss', patience=0,verbose=0, mode='auto', decayRatio=0.1):\n",
    "        super(Callback, self).__init__() \n",
    "        self.monitor = monitor\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.wait = 0\n",
    "        self.decayRatio = decayRatio\n",
    " \n",
    "        if mode not in ['auto', 'min', 'max']:\n",
    "            warnings.warn('Mode %s is unknown, '\n",
    "                          'fallback to auto mode.'\n",
    "                          % (self.mode), RuntimeWarning)\n",
    "            mode = 'auto'\n",
    " \n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            if 'acc' in self.monitor:\n",
    "                self.monitor_op = np.greater\n",
    "                self.best = -np.Inf\n",
    "            else:\n",
    "                self.monitor_op = np.less\n",
    "                self.best = np.Inf\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        current_lr = K.get_value(self.model.optimizer.lr)\n",
    "        print(\"\\nLearning rate:\", current_lr)\n",
    "        if current is None:\n",
    "            warnings.warn('AdvancedLearnignRateScheduler'\n",
    "                          ' requires %s available!' %\n",
    "                          (self.monitor), RuntimeWarning)\n",
    " \n",
    "        if self.monitor_op(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            if self.wait >= self.patience:\n",
    "                if self.verbose > 0:\n",
    "                    print('\\nEpoch %05d: reducing learning rate' % (epoch))\n",
    "                    assert hasattr(self.model.optimizer, 'lr'), \\\n",
    "                        'Optimizer must have a \"lr\" attribute.'\n",
    "                    current_lr = K.get_value(self.model.optimizer.lr)\n",
    "                    new_lr = current_lr * self.decayRatio\n",
    "                    K.set_value(self.model.optimizer.lr, new_lr)\n",
    "                    self.wait = 0 \n",
    "            self.wait += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data training set\n"
     ]
    }
   ],
   "source": [
    "#config = tf.ConfigProto(allow_soft_placement=True)\n",
    "#config.gpu_options.allow_growth = True\n",
    "#sess = tf.Session(config=config)\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Parameters\n",
    "FS = 300\n",
    "WINDOW_SIZE = 30*FS     # padding window for CNN\n",
    "\n",
    "# Loading data\n",
    "(X,y) = loaddata(WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000, 9000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calls = [\n",
    "                # Early stopping definition\n",
    "                EarlyStopping(monitor='val_loss', patience=3, verbose=1),\n",
    "                # Decrease learning rate by 0.1 factor\n",
    "                ReduceLROnPlateau(monitor='val_loss', patience=1,verbose=1, mode='min', factor=0.1),            \n",
    "                # Saving best model\n",
    "                ModelCheckpoint('weights-best_2.hdf5', monitor='val_loss', save_best_only=True, verbose=1),\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch =64\n",
    "epochs = 20\n",
    "Ntrain = X.shape[0] # number of recordings on training set\n",
    "num_valid = int(Ntrain/5) # number of recordings to take as validation        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to add dimension for training\n",
    "X = np.expand_dims(X, axis=2)\n",
    "#classes = ['A', 'N', 'O', '~']\n",
    "classes = ['N','O']\n",
    "Nclass = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000, 9000, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = ResNet_model(WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split train and validation sets\n",
    "idxval = np.random.choice(Ntrain, num_valid,replace=False)\n",
    "idxtrain = np.invert(np.in1d(range(Ntrain),idxval))\n",
    "ytrain = y[np.asarray(idxtrain),:]\n",
    "Xtrain = X[np.asarray(idxtrain),:,:]         \n",
    "Xval = X[np.asarray(idxval),:,:]\n",
    "yval = y[np.asarray(idxval),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2600, 9000, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10400 samples, validate on 2600 samples\n",
      "Epoch 1/20\n",
      "10368/10400 [============================>.] - ETA: 0s - loss: 5.5738 - acc: 0.6513Epoch 00001: val_loss improved from inf to 5.53595, saving model to weights-best_2.hdf5\n",
      "10400/10400 [==============================] - 266s 26ms/step - loss: 5.5690 - acc: 0.6516 - val_loss: 5.5359 - val_acc: 0.6565\n",
      "Epoch 2/20\n",
      "10368/10400 [============================>.] - ETA: 0s - loss: 5.5919 - acc: 0.6531Epoch 00002: val_loss did not improve\n",
      "10400/10400 [==============================] - 263s 25ms/step - loss: 5.5917 - acc: 0.6531 - val_loss: 5.5359 - val_acc: 0.6565\n",
      "Epoch 3/20\n",
      "10368/10400 [============================>.] - ETA: 0s - loss: 5.5966 - acc: 0.6528\n",
      "Epoch 00003: reducing learning rate to 0.00010000000475.\n",
      "Epoch 00003: val_loss did not improve\n",
      "10400/10400 [==============================] - 264s 25ms/step - loss: 5.5917 - acc: 0.6531 - val_loss: 5.5359 - val_acc: 0.6565\n",
      "Epoch 4/20\n",
      "10368/10400 [============================>.] - ETA: 0s - loss: 5.5919 - acc: 0.6531\n",
      "Epoch 00004: reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 00004: val_loss did not improve\n",
      "10400/10400 [==============================] - 263s 25ms/step - loss: 5.5917 - acc: 0.6531 - val_loss: 5.5359 - val_acc: 0.6565\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb5c2c1cd0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit(Xtrain, ytrain,\n",
    "          validation_data=(Xval, yval),\n",
    "          epochs=epochs, batch_size=batch,\n",
    "         callbacks=calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf_k2]",
   "language": "python",
   "name": "conda-env-tf_k2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
