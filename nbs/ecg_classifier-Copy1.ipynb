{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import gc\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sys\n",
    "sys.path.insert(0, './preparation')\n",
    "import os\n",
    "\n",
    "# Keras imports\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, Dense, Flatten, Dropout,MaxPooling1D, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback,warnings, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/ubuntu/projects/ecg/'\n",
    "data_path = path + 'data/train/training/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loaddata(WINDOW_SIZE):    \n",
    "    '''\n",
    "        Load training/test data into workspace\n",
    "        \n",
    "        This function assumes you have downloaded and padded/truncated the \n",
    "        training set into a local file named \"trainingset.mat\". This file should \n",
    "        contain the following structures:\n",
    "            - trainset: NxM matrix of N ECG segments with length M\n",
    "            - traintarget: Nx4 matrix of coded labels where each column contains\n",
    "            one in case it matches ['A', 'N', 'O', '~'].\n",
    "        \n",
    "    '''\n",
    "    print(\"Loading data training set\")        \n",
    "    matfile = scipy.io.loadmat(data_path+'trainingset_2.mat')\n",
    "    X = matfile['trainset']\n",
    "    y = matfile['traintarget']\n",
    "    \n",
    "    # Merging datasets    \n",
    "    # Case other sets are available, load them then concatenate\n",
    "    #y = np.concatenate((traintarget,augtarget),axis=0)     \n",
    "    #X = np.concatenate((trainset,augset),axis=0)     \n",
    "\n",
    "    X =  X[:,0:WINDOW_SIZE] \n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ResNet_model(WINDOW_SIZE):\n",
    "    # Add CNN layers left branch (higher frequencies)\n",
    "    # Parameters from paper\n",
    "    INPUT_FEAT = 1\n",
    "    OUTPUT_CLASS = 2    # output classes\n",
    "\n",
    "    k = 1    # increment every 4th residual block\n",
    "    p = True # pool toggle every other residual block (end with 2^8)\n",
    "    convfilt = 64\n",
    "    convstr = 1\n",
    "    kern_size = 16\n",
    "    poolsize = 2\n",
    "    poolstr  = 2\n",
    "    drop = 0.7\n",
    "    \n",
    "    # Modelling with Functional API\n",
    "    #input1 = Input(shape=(None,1), name='input')\n",
    "    input1 = Input(shape=(WINDOW_SIZE,INPUT_FEAT))\n",
    "    \n",
    "    ## First convolutional block (conv,BN, relu)\n",
    "    x = Conv1D(filters=convfilt,\n",
    "               kernel_size=kern_size,\n",
    "               padding='same',\n",
    "               strides=convstr,\n",
    "               kernel_initializer='he_normal')(input1)                \n",
    "    x = BatchNormalization()(x)        \n",
    "    x = Activation('relu')(x)  \n",
    "    \n",
    "    ## Second convolutional block (conv, BN, relu, dropout, conv) with residual net\n",
    "    # Left branch (convolutions)\n",
    "    x1 =  Conv1D(filters=convfilt,\n",
    "               kernel_size=kern_size,\n",
    "               padding='same',\n",
    "               strides=convstr,\n",
    "               kernel_initializer='he_normal')(x)      \n",
    "    x1 = BatchNormalization()(x1)    \n",
    "    x1 = Activation('relu')(x1)\n",
    "    x1 = Dropout(drop)(x1)\n",
    "    x1 =  Conv1D(filters=convfilt,\n",
    "               kernel_size=kern_size,\n",
    "               padding='same',\n",
    "               strides=convstr,\n",
    "               kernel_initializer='he_normal')(x1)\n",
    "    x1 = MaxPooling1D(pool_size=poolsize,\n",
    "                      strides=poolstr)(x1)\n",
    "    # Right branch, shortcut branch pooling\n",
    "    x2 = MaxPooling1D(pool_size=poolsize,\n",
    "                      strides=poolstr)(x)\n",
    "    # Merge both branches\n",
    "    x = keras.layers.add([x1, x2])\n",
    "    del x1,x2\n",
    "    \n",
    "    ## Main loop\n",
    "    p = not p \n",
    "    for l in range(15):\n",
    "        \n",
    "        if (l%4 == 0) and (l>0): # increment k on every fourth residual block\n",
    "            k += 1\n",
    "             # increase depth by 1x1 Convolution case dimension shall change\n",
    "            xshort = Conv1D(filters=convfilt*k,kernel_size=1)(x)\n",
    "        else:\n",
    "            xshort = x        \n",
    "        # Left branch (convolutions)\n",
    "        # notice the ordering of the operations has changed        \n",
    "        x1 = BatchNormalization()(x)\n",
    "        x1 = Activation('relu')(x1)\n",
    "        x1 = Dropout(drop)(x1)\n",
    "        x1 =  Conv1D(filters=convfilt*k,\n",
    "               kernel_size=kern_size,\n",
    "               padding='same',\n",
    "               strides=convstr,\n",
    "               kernel_initializer='he_normal')(x1)        \n",
    "        x1 = BatchNormalization()(x1)\n",
    "        x1 = Activation('relu')(x1)\n",
    "        x1 = Dropout(drop)(x1)\n",
    "        x1 =  Conv1D(filters=convfilt*k,\n",
    "               kernel_size=kern_size,\n",
    "               padding='same',\n",
    "               strides=convstr,\n",
    "               kernel_initializer='he_normal')(x1)        \n",
    "        if p:\n",
    "            x1 = MaxPooling1D(pool_size=poolsize,strides=poolstr)(x1)                \n",
    "\n",
    "        # Right branch: shortcut connection\n",
    "        if p:\n",
    "            x2 = MaxPooling1D(pool_size=poolsize,strides=poolstr)(xshort)\n",
    "        else:\n",
    "            x2 = xshort  # pool or identity            \n",
    "        # Merging branches\n",
    "        x = keras.layers.add([x1, x2])\n",
    "        # change parameters\n",
    "        p = not p # toggle pooling\n",
    "\n",
    "    \n",
    "    # Final bit    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x) \n",
    "    x = Flatten()(x)\n",
    "    #x = Dense(1000)(x)\n",
    "    #x = Dense(1000)(x)\n",
    "    out = Dense(OUTPUT_CLASS, activation='softmax')(x)\n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    #sequential_model_to_ascii_printout(model)\n",
    "    #plot_model(model, to_file='model.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AdvancedLearnignRateScheduler(Callback):    \n",
    "    '''\n",
    "   # Arguments\n",
    "       monitor: quantity to be monitored.\n",
    "       patience: number of epochs with no improvement\n",
    "           after which training will be stopped.\n",
    "       verbose: verbosity mode.\n",
    "       mode: one of {auto, min, max}. In 'min' mode,\n",
    "           training will stop when the quantity\n",
    "           monitored has stopped decreasing; in 'max'\n",
    "           mode it will stop when the quantity\n",
    "           monitored has stopped increasing.\n",
    "   '''\n",
    "    def __init__(self, monitor='val_loss', patience=0,verbose=0, mode='auto', decayRatio=0.1):\n",
    "        super(Callback, self).__init__() \n",
    "        self.monitor = monitor\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.wait = 0\n",
    "        self.decayRatio = decayRatio\n",
    " \n",
    "        if mode not in ['auto', 'min', 'max']:\n",
    "            warnings.warn('Mode %s is unknown, '\n",
    "                          'fallback to auto mode.'\n",
    "                          % (self.mode), RuntimeWarning)\n",
    "            mode = 'auto'\n",
    " \n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            if 'acc' in self.monitor:\n",
    "                self.monitor_op = np.greater\n",
    "                self.best = -np.Inf\n",
    "            else:\n",
    "                self.monitor_op = np.less\n",
    "                self.best = np.Inf\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        current_lr = K.get_value(self.model.optimizer.lr)\n",
    "        print(\"\\nLearning rate:\", current_lr)\n",
    "        if current is None:\n",
    "            warnings.warn('AdvancedLearnignRateScheduler'\n",
    "                          ' requires %s available!' %\n",
    "                          (self.monitor), RuntimeWarning)\n",
    " \n",
    "        if self.monitor_op(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            if self.wait >= self.patience:\n",
    "                if self.verbose > 0:\n",
    "                    print('\\nEpoch %05d: reducing learning rate' % (epoch))\n",
    "                    assert hasattr(self.model.optimizer, 'lr'), \\\n",
    "                        'Optimizer must have a \"lr\" attribute.'\n",
    "                    current_lr = K.get_value(self.model.optimizer.lr)\n",
    "                    new_lr = current_lr * self.decayRatio\n",
    "                    K.set_value(self.model.optimizer.lr, new_lr)\n",
    "                    self.wait = 0 \n",
    "            self.wait += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data training set\n"
     ]
    }
   ],
   "source": [
    "#config = tf.ConfigProto(allow_soft_placement=True)\n",
    "#config.gpu_options.allow_growth = True\n",
    "#sess = tf.Session(config=config)\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Parameters\n",
    "FS = 300\n",
    "WINDOW_SIZE = 30*FS     # padding window for CNN\n",
    "\n",
    "# Loading data\n",
    "(X,y) = loaddata(WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8528, 9000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calls = [\n",
    "                # Early stopping definition\n",
    "                EarlyStopping(monitor='val_loss', patience=3, verbose=1),\n",
    "                # Decrease learning rate by 0.1 factor\n",
    "                ReduceLROnPlateau(monitor='val_loss', patience=1,verbose=1, mode='min', factor=0.1),            \n",
    "                # Saving best model\n",
    "                ModelCheckpoint('weights-best_2.hdf5', monitor='val_loss', save_best_only=True, verbose=1),\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch =64\n",
    "epochs = 20\n",
    "Ntrain = X.shape[0] # number of recordings on training set\n",
    "num_valid = int(Ntrain/5) # number of recordings to take as validation        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to add dimension for training\n",
    "X = np.expand_dims(X, axis=2)\n",
    "#classes = ['A', 'N', 'O', '~']\n",
    "classes = ['N','O']\n",
    "Nclass = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8528, 9000, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = ResNet_model(WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split train and validation sets\n",
    "idxval = np.random.choice(Ntrain, num_valid,replace=False)\n",
    "idxtrain = np.invert(np.in1d(range(Ntrain),idxval))\n",
    "ytrain = y[np.asarray(idxtrain),:]\n",
    "Xtrain = X[np.asarray(idxtrain),:,:]         \n",
    "Xval = X[np.asarray(idxval),:,:]\n",
    "yval = y[np.asarray(idxval),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1705, 9000, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6823 samples, validate on 1705 samples\n",
      "Epoch 1/20\n",
      "6784/6823 [============================>.] - ETA: 1s - loss: 0.9108 - acc: 0.5439Epoch 00001: val_loss improved from 6.26762 to 1.07919, saving model to weights-best_2.hdf5\n",
      "6823/6823 [==============================] - 279s 41ms/step - loss: 0.9110 - acc: 0.5437 - val_loss: 1.0792 - val_acc: 0.4809\n",
      "Epoch 2/20\n",
      "6784/6823 [============================>.] - ETA: 1s - loss: 0.7701 - acc: 0.6010Epoch 00002: val_loss improved from 1.07919 to 0.71009, saving model to weights-best_2.hdf5\n",
      "6823/6823 [==============================] - 267s 39ms/step - loss: 0.7694 - acc: 0.6019 - val_loss: 0.7101 - val_acc: 0.6370\n",
      "Epoch 3/20\n",
      "6784/6823 [============================>.] - ETA: 1s - loss: 0.6912 - acc: 0.6806Epoch 00003: val_loss did not improve\n",
      "6823/6823 [==============================] - 265s 39ms/step - loss: 0.6897 - acc: 0.6809 - val_loss: 1.4742 - val_acc: 0.6282\n",
      "Epoch 4/20\n",
      "6784/6823 [============================>.] - ETA: 1s - loss: 0.5337 - acc: 0.7633Epoch 00004: val_loss improved from 0.71009 to 0.45210, saving model to weights-best_2.hdf5\n",
      "6823/6823 [==============================] - 267s 39ms/step - loss: 0.5321 - acc: 0.7643 - val_loss: 0.4521 - val_acc: 0.8076\n",
      "Epoch 5/20\n",
      "6784/6823 [============================>.] - ETA: 1s - loss: 0.4702 - acc: 0.8013Epoch 00005: val_loss did not improve\n",
      "6823/6823 [==============================] - 265s 39ms/step - loss: 0.4712 - acc: 0.8013 - val_loss: 0.4628 - val_acc: 0.8258\n",
      "Epoch 6/20\n",
      "6784/6823 [============================>.] - ETA: 1s - loss: 0.4380 - acc: 0.8166\n",
      "Epoch 00006: reducing learning rate to 0.00010000000475.\n",
      "Epoch 00006: val_loss did not improve\n",
      "6823/6823 [==============================] - 266s 39ms/step - loss: 0.4388 - acc: 0.8161 - val_loss: 0.5956 - val_acc: 0.7765\n",
      "Epoch 7/20\n",
      "6784/6823 [============================>.] - ETA: 1s - loss: 0.3583 - acc: 0.8545Epoch 00007: val_loss improved from 0.45210 to 0.41736, saving model to weights-best_2.hdf5\n",
      "6823/6823 [==============================] - 267s 39ms/step - loss: 0.3578 - acc: 0.8546 - val_loss: 0.4174 - val_acc: 0.8293\n",
      "Epoch 8/20\n",
      "6784/6823 [============================>.] - ETA: 1s - loss: 0.3357 - acc: 0.8638Epoch 00008: val_loss did not improve\n",
      "6823/6823 [==============================] - 265s 39ms/step - loss: 0.3351 - acc: 0.8638 - val_loss: 0.4186 - val_acc: 0.8323\n",
      "Epoch 9/20\n",
      "6784/6823 [============================>.] - ETA: 1s - loss: 0.3272 - acc: 0.8685Epoch 00009: val_loss improved from 0.41736 to 0.40693, saving model to weights-best_2.hdf5\n",
      "6823/6823 [==============================] - 267s 39ms/step - loss: 0.3272 - acc: 0.8685 - val_loss: 0.4069 - val_acc: 0.8375\n",
      "Epoch 10/20\n",
      "6784/6823 [============================>.] - ETA: 1s - loss: 0.3221 - acc: 0.8698Epoch 00010: val_loss did not improve\n",
      "6823/6823 [==============================] - 265s 39ms/step - loss: 0.3211 - acc: 0.8704 - val_loss: 0.4141 - val_acc: 0.8358\n",
      "Epoch 11/20\n",
      "6784/6823 [============================>.] - ETA: 1s - loss: 0.3147 - acc: 0.8728\n",
      "Epoch 00011: reducing learning rate to 1.0000000475e-05.\n",
      "Epoch 00011: val_loss did not improve\n",
      "6823/6823 [==============================] - 265s 39ms/step - loss: 0.3152 - acc: 0.8728 - val_loss: 0.4179 - val_acc: 0.8352\n",
      "Epoch 12/20\n",
      "6784/6823 [============================>.] - ETA: 1s - loss: 0.3003 - acc: 0.8805\n",
      "Epoch 00012: reducing learning rate to 1.00000006569e-06.\n",
      "Epoch 00012: val_loss did not improve\n",
      "6823/6823 [==============================] - 265s 39ms/step - loss: 0.3000 - acc: 0.8806 - val_loss: 0.4206 - val_acc: 0.8334\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f729e473750>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit(Xtrain, ytrain,\n",
    "          validation_data=(Xval, yval),\n",
    "          epochs=epochs, batch_size=batch,\n",
    "         callbacks=calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_eval(X,y):\n",
    "    batch =64\n",
    "    epochs = 20  \n",
    "    rep = 1         # K fold procedure can be repeated multiple times\n",
    "    Kfold = 5\n",
    "    Ntrain = 8528 # number of recordings on training set\n",
    "    num_valid = int(Ntrain/Kfold) # number of recordings to take as validation        \n",
    "   \n",
    "    # Need to add dimension for training\n",
    "    X = np.expand_dims(X, axis=2)\n",
    "    classes = ['A', 'N', 'O', '~']\n",
    "    Nclass = len(classes)\n",
    "    cvconfusion = np.zeros((Nclass,Nclass,Kfold*rep))\n",
    "    cvscores = []       \n",
    "    counter = 0\n",
    "    # repetitions of cross validation\n",
    "    for r in range(rep):\n",
    "        print(\"Rep %d\"%(r+1))\n",
    "        # cross validation loop\n",
    "        for k in range(Kfold):\n",
    "            print(\"Cross-validation run %d\"%(k+1))\n",
    "            # Callbacks definition\n",
    "            callbacks = [\n",
    "                # Early stopping definition\n",
    "                EarlyStopping(monitor='val_loss', patience=3, verbose=1),\n",
    "                # Decrease learning rate by 0.1 factor\n",
    "                AdvancedLearnignRateScheduler(monitor='val_loss', patience=1,verbose=1, mode='auto', decayRatio=0.1),            \n",
    "                # Saving best model\n",
    "                ModelCheckpoint('primit-weights-best_k{}_r{}.hdf5'.format(k,r), monitor='val_loss', save_best_only=True, verbose=1),\n",
    "                ]\n",
    "            # Load model\n",
    "            model = ResNet_model(WINDOW_SIZE)\n",
    "            \n",
    "            # split train and validation sets\n",
    "            idxval = np.random.choice(Ntrain, num_valid,replace=False)\n",
    "            idxtrain = np.invert(np.in1d(range(X_train.shape[0]),idxval))\n",
    "            ytrain = y[np.asarray(idxtrain),:]\n",
    "            Xtrain = X[np.asarray(idxtrain),:,:]         \n",
    "            Xval = X[np.asarray(idxval),:,:]\n",
    "            yval = y[np.asarray(idxval),:]\n",
    "            \n",
    "            # Train model\n",
    "            model.fit(Xtrain, ytrain,\n",
    "                      validation_data=(Xval, yval),\n",
    "                      epochs=epochs, batch_size=batch,callbacks=callbacks)\n",
    "            \n",
    "            # Evaluate best trained model\n",
    "            model.load_weights('weights-best_k{}_r{}.hdf5'.format(k,r))\n",
    "            ypred = model.predict(Xval)\n",
    "            ypred = np.argmax(ypred,axis=1)\n",
    "            ytrue = np.argmax(yval,axis=1)\n",
    "            cvconfusion[:,:,counter] = confusion_matrix(ytrue, ypred)\n",
    "            F1 = np.zeros((4,1))\n",
    "            for i in range(4):\n",
    "                F1[i]=2*cvconfusion[i,i,counter]/(np.sum(cvconfusion[i,:,counter])+np.sum(cvconfusion[:,i,counter]))\n",
    "                print(\"F1 measure for {} rhythm: {:1.4f}\".format(classes[i],F1[i,0]))            \n",
    "            cvscores.append(np.mean(F1)* 100)\n",
    "            print(\"Overall F1 measure: {:1.4f}\".format(np.mean(F1)))            \n",
    "            K.clear_session()\n",
    "            gc.collect()\n",
    "            #config = tf.ConfigProto()\n",
    "            #config.gpu_options.allow_growth=True            \n",
    "            #sess = tf.Session(config=config)\n",
    "            #K.set_session(sess)\n",
    "            counter += 1\n",
    "    # Saving cross validation results \n",
    "    scipy.io.savemat('xval_results.mat',mdict={'cvconfusion': cvconfusion.tolist()})  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    cm = np.around(cm, decimals=3)\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig('confusion.eps', format='eps', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#config = tf.ConfigProto(allow_soft_placement=True)\n",
    "#config.gpu_options.allow_growth = True\n",
    "#sess = tf.Session(config=config)\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Parameters\n",
    "FS = 300\n",
    "WINDOW_SIZE = 30*FS     # padding window for CNN\n",
    "\n",
    "# Loading data\n",
    "(X_train,y_train) = loaddata(WINDOW_SIZE)\n",
    "\n",
    "# Training model\n",
    "model = model_eval(X_train,y_train)\n",
    "\n",
    "# Outputing results of cross validation\n",
    "matfile = scipy.io.loadmat('xval_results.mat')\n",
    "cv = matfile['cvconfusion']\n",
    "F1mean = np.zeros(cv.shape[2])\n",
    "for j in range(cv.shape[2]):\n",
    "    classes = ['A', 'N', 'O', '~']\n",
    "    F1 = np.zeros((4,1))\n",
    "    for i in range(4):\n",
    "        F1[i]=2*cv[i,i,j]/(np.sum(cv[i,:,j])+np.sum(cv[:,i,j]))        \n",
    "        print(\"F1 measure for {} rhythm: {:1.4f}\".format(classes[i],F1[i,0]))\n",
    "    F1mean[j] = np.mean(F1)\n",
    "    print(\"mean F1 measure for: {:1.4f}\".format(F1mean[j]))\n",
    "print(\"Overall F1 : {:1.4f}\".format(np.mean(F1mean)))\n",
    "# Plotting confusion matrix\n",
    "cvsum = np.sum(cv,axis=2)\n",
    "for i in range(4):\n",
    "    F1[i]=2*cvsum[i,i]/(np.sum(cvsum[i,:])+np.sum(cvsum[:,i]))        \n",
    "    print(\"F1 measure for {} rhythm: {:1.4f}\".format(classes[i],F1[i,0]))\n",
    "F1mean = np.mean(F1)\n",
    "print(\"mean F1 measure for: {:1.4f}\".format(F1mean))\n",
    "plot_confusion_matrix(cvsum, classes,normalize=True,title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "INPUT_FEAT = 1\n",
    "input1 = Input(shape=(WINDOW_SIZE,INPUT_FEAT), name='input')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf_k2]",
   "language": "python",
   "name": "conda-env-tf_k2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
